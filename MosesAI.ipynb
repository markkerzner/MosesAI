{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MosesAI project\n",
    "\n",
    "1. Read all Talmud pages in a directory\n",
    "2. Send them to Pinecode\n",
    "3. Read Pinecone index\n",
    "4. For a query, find relevant documents\n",
    "5. Using Langchain, send the query and relevant documents to ChatGPT\n",
    "6. Get the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')\n",
    "MODEL=\"gpt-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "C_-uyi19vNjG",
    "outputId": "6f1fea63-559c-4763-b120-cbbd81a8291a"
   },
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "#from PIL import Image               # to load images\n",
    "#from IPython.display import display # to display images\n",
    "#pil_im = Image.open('/content/langchain and pinecone.png')\n",
    "#display(pil_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HwalYTVZoRlH",
    "outputId": "9f475bea-721c-479f-c1fb-ea0ad263c9b7"
   },
   "outputs": [],
   "source": [
    "#!apt-get install poppler-utils  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AssnWUVlxtH"
   },
   "source": [
    "https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/directory_loader.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fulYnj9nZr3n",
    "outputId": "2b9f6003-0b72-407a-de44-df9b7e216890"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2297"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "directory = 'data/talmud-pages/'\n",
    "\n",
    "def load_docs(directory):\n",
    "  loader = DirectoryLoader(directory)\n",
    "  documents = loader.load()\n",
    "  return documents\n",
    "\n",
    "documents = load_docs(directory)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwaCosqsogzw"
   },
   "source": [
    "https://python.langchain.com/en/latest/modules/indexes/text_splitters/getting_started.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5lF8jA6xZ0Hm",
    "outputId": "7fd4a04f-f80d-465b-a028-e8a611ebb9f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3929\n",
      "2297\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_docs(documents,chunk_size=1000,chunk_overlap=20):\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "  docs = text_splitter.split_documents(documents)\n",
    "  return docs\n",
    "\n",
    "docs = split_docs(documents)\n",
    "print(len(docs))\n",
    "\n",
    "docs = documents\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tVcFjgHJZ8S9",
    "outputId": "c8011cb1-9cd9-4223-ad71-60bbc83f9cbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nazir 9 - Nazir who did not like figs If one says, \"I am a nazir, so I cannot eat figs,\" - this is a strange statement. Being a nazir means specifically abstaining from grapes, nothing else. However, Beit Shammai says that he does become a nazir nevertheless. How so? People usually do not make nonsensical statements. This one probably wanted to become a nazir but added that he meant figs. He could have made a mistake, thinking there was such a thing. Or, he really could have changed his mind and was preparing a loophole for himself. But the problem is that Beit Shammai does not accept the idea of changing one's mind regarding Temple-related things. So either way, he becomes a nazir. What about Beit Hillel? They say that the man is not a nazir. He made a statement, that is true, but it was not a valid legal statement about becoming a nazir. So it did not take effect at all. Art: Melon And Bowl Of Figs by Gustave Caillebotte Talk to MosesAI about it\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F5GY9voPa0av",
    "outputId": "5fd540c3-8e46-4863-aa07-8fcde3e0d822"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_287892/699783191.py:4: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key)\n",
    "\n",
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vySq5oI5sU5V"
   },
   "source": [
    "https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/pinecone.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hfIpYLV-acks"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notebook cell â€• Pinecone 3.x + LangChain 0.2\n",
    "-------------------------------------------\n",
    "Requires:\n",
    "  pip install --upgrade pinecone langchain-pinecone langchain-openai\n",
    "Environment:\n",
    "  OPENAI_API_KEY, PINECONE_API_KEY\n",
    "Inputs:\n",
    "  docs  # list[langchain.schema.Document] if you plan to ingest\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# 1) Embedding model\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# 2) Decide the vector dimension\n",
    "DIMENSION = embeddings.dimensions or 1536   # text-embedding-3-small default :contentReference[oaicite:0]{index=0}\n",
    "\n",
    "# 3) Connect to Pinecone\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "index_name = \"talmud-pages\"\n",
    "\n",
    "# 4) Create the index once\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=DIMENSION,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"gcp\", region=\"us-central1\")\n",
    "    )\n",
    "\n",
    "# 5) Open the index and wrap it for LangChain\n",
    "pc_index = pc.Index(index_name)\n",
    "vectorstore = PineconeVectorStore(index=pc_index, embedding=embeddings)\n",
    "\n",
    "# To ingest data if the index is empty:\n",
    "# vectorstore.add_documents(docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o5r7YLpbchAD",
    "outputId": "0433cb24-6adb-4daa-d629-337669379a0c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m similar_docs\n\u001b[32m      8\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mWhen do you say Shema?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m similar_docs = \u001b[43mget_similiar_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mlen\u001b[39m(similar_docs)\n\u001b[32m     11\u001b[39m similar_docs[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mget_similiar_docs\u001b[39m\u001b[34m(query, k, score)\u001b[39m\n\u001b[32m      3\u001b[39m   similar_docs = index.similarity_search_with_score(query,k=k)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m   similar_docs = \u001b[43mindex\u001b[49m.similarity_search(query,k=k)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m similar_docs\n",
      "\u001b[31mNameError\u001b[39m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "def get_similiar_docs(query,k=5,score=False):\n",
    "  if score:\n",
    "    similar_docs = index.similarity_search_with_score(query,k=k)\n",
    "  else:\n",
    "    similar_docs = index.similarity_search(query,k=k)\n",
    "  return similar_docs\n",
    "\n",
    "query = \"When do you say Shema?\"\n",
    "similar_docs = get_similiar_docs(query)\n",
    "len(similar_docs)\n",
    "similar_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtF9QZSvbLD9"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DuevPx4dbI4W",
    "outputId": "76a1ed52-c785-40a2-a0e9-828837972c18"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvqmkpJvss17"
   },
   "source": [
    "https://python.langchain.com/en/latest/use_cases/question_answering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "RqCE-C3Ubty0",
    "outputId": "f78b567f-5483-4ea9-90fb-3698ce1bf325"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "def get_answer(query):\n",
    "  similar_docs = get_similiar_docs(query)\n",
    "  # print(similar_docs)\n",
    "  answer =  chain.run(input_documents=similar_docs, question=query)\n",
    "  return  answer\n",
    "\n",
    "query = \"When to say Shema?\"  \n",
    "get_answer(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "RTnCqRJ3r1kf",
    "outputId": "07b97326-64bf-4e25-9f61-a2eef3081784"
   },
   "outputs": [],
   "source": [
    "query = \"what are the sacrifices for? \\\n",
    "Sacrifices are typically brought for mistakes or unintentional transgressions, as stated in Keritot 9. \\\n",
    "However, there are cases when one brings an offering for intentional acts, such as relations with a slavewoman designated for another, a nazir who went to the cemetery, and one who swore a false oath of testimony (also mentioned in Keritot 9) \\\n",
    "what about bird sacrifices?\"\n",
    "get_answer(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM+LlDQsGXPEHE1XTQC7Kf0",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
